{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result_for_label(label, start_idx=0, end_idx=10):\n",
    "    full_info_list = []\n",
    "    for run_idx in range(start_idx, end_idx):\n",
    "        with open(f'{label}_{run_idx}.json') as f:\n",
    "            autosd_results = json.load(f)\n",
    "        for arhe_idx, autosd_apr_res in enumerate(autosd_results):\n",
    "            bug_name = autosd_apr_res['task_id']\n",
    "            autosd_repair_succ = autosd_apr_res['passed']\n",
    "            if 'trace' not in autosd_apr_res:\n",
    "                continue\n",
    "            synth_process = autosd_apr_res['trace'].split('## Analysis')[-1]\n",
    "            debugging_done = '<DEBUGGING DONE>' in synth_process\n",
    "            succ_hypothesis = 'The hypothesis is supported' in synth_process or 'The hypothesis was supported' in synth_process\n",
    "            full_info_list.append({\n",
    "                'run_idx': run_idx,\n",
    "                'humaneval_name': bug_name,\n",
    "                'arhe_idx': arhe_idx,\n",
    "                'autosd_repair_succ': autosd_repair_succ,\n",
    "                'debugging_done': debugging_done,\n",
    "                'succ_hypothesis': succ_hypothesis,\n",
    "            })\n",
    "    return pandas.DataFrame(full_info_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoSD Fixed: 189\n",
      "Acc of Done: 0.8188277087033747\n",
      "Acc of Not Done: 0.6941508104298802\n"
     ]
    }
   ],
   "source": [
    "# headline results\n",
    "label = './results/chatgpt002_ZSDloopV4_s3_out'\n",
    "full_info_df = get_result_for_label(label)\n",
    "print('AutoSD Fixed:', full_info_df[full_info_df.autosd_repair_succ]['arhe_idx'].nunique())\n",
    "\n",
    "print('Acc of Done:', (full_info_df[full_info_df.debugging_done]['autosd_repair_succ'].sum()/\n",
    "                      full_info_df.debugging_done.sum()))\n",
    "print('Acc of Not Done:', (full_info_df[~full_info_df.debugging_done]['autosd_repair_succ'].sum()/\n",
    "                           (~full_info_df.debugging_done).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mutant</th>\n",
       "      <th>mut_op</th>\n",
       "      <th>task_id</th>\n",
       "      <th>failed_tests</th>\n",
       "      <th>samples</th>\n",
       "      <th>trace</th>\n",
       "      <th>prompt_at_repair</th>\n",
       "      <th>passed</th>\n",
       "      <th>fail_tests</th>\n",
       "      <th>ARHE_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>def intersperse(numbers: List[int], delimeter:...</td>\n",
       "      <td>_int_const_changer</td>\n",
       "      <td>HumanEval/5</td>\n",
       "      <td>[{'failing_assertion': '    assert candidate([...</td>\n",
       "      <td>[\\ndef intersperse(numbers: List[int], delimet...</td>\n",
       "      <td># Debugging Log\\n\\nAuthored by Linus Torvalds....</td>\n",
       "      <td># Debugging Log\\n\\nAuthored by Linus Torvalds....</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>def longest(strings: List[str]) -&gt; Optional[st...</td>\n",
       "      <td>_if_remover</td>\n",
       "      <td>HumanEval/12</td>\n",
       "      <td>[{'failing_assertion': '    assert candidate([...</td>\n",
       "      <td>[\\ndef longest(strings: List[str]) -&gt; Optional...</td>\n",
       "      <td># Debugging Log\\n\\nAuthored by Linus Torvalds....</td>\n",
       "      <td># Debugging Log\\n\\nAuthored by Linus Torvalds....</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>def sort_numbers(numbers: str) -&gt; str:\\n    \"\"...</td>\n",
       "      <td>_int_const_changer</td>\n",
       "      <td>HumanEval/19</td>\n",
       "      <td>[{'failing_assertion': '    assert candidate('...</td>\n",
       "      <td>[\\ndef sort_numbers(numbers: str) -&gt; str:\\n   ...</td>\n",
       "      <td># Debugging Log\\n\\nAuthored by Linus Torvalds....</td>\n",
       "      <td># Debugging Log\\n\\nAuthored by Linus Torvalds....</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>def sort_numbers(numbers: str) -&gt; str:\\n    \"\"...</td>\n",
       "      <td>_int_const_changer</td>\n",
       "      <td>HumanEval/19</td>\n",
       "      <td>[{'failing_assertion': '    assert candidate('...</td>\n",
       "      <td>[\\ndef sort_numbers(numbers: str) -&gt; str:\\n   ...</td>\n",
       "      <td># Debugging Log\\n\\nAuthored by Linus Torvalds....</td>\n",
       "      <td># Debugging Log\\n\\nAuthored by Linus Torvalds....</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>def sort_numbers(numbers: str) -&gt; str:\\n    \"\"...</td>\n",
       "      <td>_str_const_changer</td>\n",
       "      <td>HumanEval/19</td>\n",
       "      <td>[{'failing_assertion': '    assert candidate('...</td>\n",
       "      <td>[\\ndef sort_numbers(numbers: str) -&gt; str:\\n  v...</td>\n",
       "      <td># Debugging Log\\n\\nAuthored by Linus Torvalds....</td>\n",
       "      <td># Debugging Log\\n\\nAuthored by Linus Torvalds....</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              mutant              mut_op  \\\n",
       "0  def intersperse(numbers: List[int], delimeter:...  _int_const_changer   \n",
       "1  def longest(strings: List[str]) -> Optional[st...         _if_remover   \n",
       "2  def sort_numbers(numbers: str) -> str:\\n    \"\"...  _int_const_changer   \n",
       "3  def sort_numbers(numbers: str) -> str:\\n    \"\"...  _int_const_changer   \n",
       "4  def sort_numbers(numbers: str) -> str:\\n    \"\"...  _str_const_changer   \n",
       "\n",
       "        task_id                                       failed_tests  \\\n",
       "0   HumanEval/5  [{'failing_assertion': '    assert candidate([...   \n",
       "1  HumanEval/12  [{'failing_assertion': '    assert candidate([...   \n",
       "2  HumanEval/19  [{'failing_assertion': '    assert candidate('...   \n",
       "3  HumanEval/19  [{'failing_assertion': '    assert candidate('...   \n",
       "4  HumanEval/19  [{'failing_assertion': '    assert candidate('...   \n",
       "\n",
       "                                             samples  \\\n",
       "0  [\\ndef intersperse(numbers: List[int], delimet...   \n",
       "1  [\\ndef longest(strings: List[str]) -> Optional...   \n",
       "2  [\\ndef sort_numbers(numbers: str) -> str:\\n   ...   \n",
       "3  [\\ndef sort_numbers(numbers: str) -> str:\\n   ...   \n",
       "4  [\\ndef sort_numbers(numbers: str) -> str:\\n  v...   \n",
       "\n",
       "                                               trace  \\\n",
       "0  # Debugging Log\\n\\nAuthored by Linus Torvalds....   \n",
       "1  # Debugging Log\\n\\nAuthored by Linus Torvalds....   \n",
       "2  # Debugging Log\\n\\nAuthored by Linus Torvalds....   \n",
       "3  # Debugging Log\\n\\nAuthored by Linus Torvalds....   \n",
       "4  # Debugging Log\\n\\nAuthored by Linus Torvalds....   \n",
       "\n",
       "                                    prompt_at_repair  passed fail_tests  \\\n",
       "0  # Debugging Log\\n\\nAuthored by Linus Torvalds....    True         []   \n",
       "1  # Debugging Log\\n\\nAuthored by Linus Torvalds....    True         []   \n",
       "2  # Debugging Log\\n\\nAuthored by Linus Torvalds....    True         []   \n",
       "3  # Debugging Log\\n\\nAuthored by Linus Torvalds....    True         []   \n",
       "4  # Debugging Log\\n\\nAuthored by Linus Torvalds....    True         []   \n",
       "\n",
       "   ARHE_id  \n",
       "0        0  \n",
       "1        1  \n",
       "2        2  \n",
       "3        3  \n",
       "4        4  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json(\"/data/wyk/autosd/arhe/results/chatgpt002_ZSDloopV4_s3_out_0.json\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.loc[4, 'fail_tests'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'failing_assertion': \"    assert candidate('three') == 'three'\", 'failing_exception': \"<class 'SyntaxError'>\"}\n"
     ]
    }
   ],
   "source": [
    "print(df.loc[4, 'fail_tests'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Debugging Log\n",
      "\n",
      "Authored by Linus Torvalds.\n",
      "\n",
      "I use the _scientific method_ to debug the problem below (as written by Zeller, 2009) by describing the hypothesis/prediction/experiment/observation/conclusion. This can be done by:\n",
      "\n",
      "Hypothesis: An explanation for the buggy behavior behavior. Hypotheses are the key aspect of the approach, and should be detailed and written with care. Hypotheses should build upon all previous information; repeating previous hypotheses is thus strongly discouraged. Some examples are provided below.\n",
      "\n",
      " - Example hypothesis 1: \"Given that [information], the method is [overall erroneous behavior]. Specifically, I think it is because `c>b` on line 4321 of method `foo` is intended to [desired behavior], but is [erroneous behavior].\"\n",
      " - Example hypothesis 2: \"The previous hypothesis was rejected. we now know `c>b` on line 4321 of the method `foo` is likely not the culprit. Looking elsewhere, perhaps `x.append(y)` on line 4323 of `foo` should do [desired behavior], but is doing [erroneous behavior].\"\n",
      " - Example hypothesis 3: \"The previous hypothesis was supported. I think changing the code by changing `c>b` to `c>b && a <= d` may fix the code.\"\n",
      " - Example hypothesis 4: \"The previous experiment ended in an error. We may need to try a different experiment. Perhaps the experiment can be refined by inspecting the value of [expression] at line [line].\"\n",
      "\n",
      "Prediction: A specific value or symptom that would be observed if the hypothesis is correct. Depending on the hypothesis, one may make the prediction that a test will pass. Make specific considerations for expressions within loops.\n",
      "\n",
      " - Example 1: If I use the debugger to print [expr], while given the input and its intended role indicates that its value should be [desired value], it will be [erroneous value]; that is, when I stop the debugger at line lineno, `[expr] == [actual_value]` will be true.\n",
      " - Example 2: If I change [expr] to [new_expr], the test will pass.\n",
      "\n",
      "Experiment: A specific script that would check whether the prediction made is true. Make sure the line points to an actual statement (not a bracket).\n",
      "\n",
      " - Example 1 (pdb script): `b debugging.py:lineno ;; c ;; p x != v`\n",
      " - Example 2 (edit script): `REPLACE(4321, \"c>b\", \"c>b && a <= d\") AND RUN`\n",
      "\n",
      "Observation: The output of the script. Example: `False`\n",
      "\n",
      "Conclusion: A judgement whether the hypothesis is true based on the observation. Also add <DEBUGGING DONE> when the hypothesis confirmed leads to a concrete program fix. \n",
      " - Example: The hypothesis is (supported/rejected/undecided due to experiment error). [When a test passed, add <DEBUGGING DONE>.]\n",
      "\n",
      "## Description\n",
      "\n",
      "I have the following python function and test (at debugging.py)\n",
      "\n",
      "```python\n",
      "1 \n",
      "2 \n",
      "3 def is_prime(n):\n",
      "4     \"\"\"Return true if a given number is prime, and false otherwise.\n",
      "5     >>> is_prime(6)\n",
      "6     False\n",
      "7     >>> is_prime(101)\n",
      "8     True\n",
      "9     >>> is_prime(11)\n",
      "10     True\n",
      "11     >>> is_prime(13441)\n",
      "12     True\n",
      "13     >>> is_prime(61)\n",
      "14     True\n",
      "15     >>> is_prime(4)\n",
      "16     False\n",
      "17     >>> is_prime(1)\n",
      "18     False\n",
      "19     \"\"\"\n",
      "20     if n < 2:\n",
      "21         return 1\n",
      "22     for k in range(2, n - 1):\n",
      "23         if n % k == 0:\n",
      "24             return False\n",
      "25     return 1\n",
      "26 assert is_prime(1) == False, is_prime(1)\n",
      "```\n",
      "\n",
      "which fails with the error message\n",
      "```\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sungmin/Documents/work/humaneval-mutator/asd/debugging.py\", line 26, in <module>\n",
      "    assert is_prime(1) == False, is_prime(1)\n",
      "AssertionError: 1\n",
      "```\n",
      "\n",
      "## Analysis\n",
      "\n",
      "Using the scientific method, I debugged the issue as follows.\n",
      "\n",
      "Attempt 1.\n",
      "Hypothesis: Given that the function is intended to return False when n=1, the function is returning True when n=1. Specifically, I think it is because `return 1` on line 21 is intended to return False, but is returning True.\n",
      "Prediction: `1` will be printed.\n",
      "Experiment: `b debugging.py:21 ;; c ;; p n`\n",
      "Observation: `1`\n",
      "Conclusion: The hypothesis is supported.\n",
      "\n",
      "Attempt 2.\n",
      "Hypothesis: Given that the function is intended to return False when n=1, the function is returning True when n=1. Specifically, I think it is because `return 1` on line 21 is intended to return False, but is returning True.\n",
      "Prediction: `1` will be printed.\n",
      "Experiment: `REPLACE(21, \"return 1\", \"return False\") AND RUN`\n",
      "Observation: `[No exception triggered]`\n",
      "Conclusion: The hypothesis is supported. <DEBUGGING DONE>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df.loc[16, 'trace'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Debugging Log\n",
      "\n",
      "Authored by Linus Torvalds.\n",
      "\n",
      "I use the _scientific method_ to debug the problem below (as written by Zeller, 2009) by describing the hypothesis/prediction/experiment/observation/conclusion. This can be done by:\n",
      "\n",
      "Hypothesis: An explanation for the buggy behavior behavior. Hypotheses are the key aspect of the approach, and should be detailed and written with care. Hypotheses should build upon all previous information; repeating previous hypotheses is thus strongly discouraged. Some examples are provided below.\n",
      "\n",
      " - Example hypothesis 1: \"Given that [information], the method is [overall erroneous behavior]. Specifically, I think it is because `c>b` on line 4321 of method `foo` is intended to [desired behavior], but is [erroneous behavior].\"\n",
      " - Example hypothesis 2: \"The previous hypothesis was rejected. we now know `c>b` on line 4321 of the method `foo` is likely not the culprit. Looking elsewhere, perhaps `x.append(y)` on line 4323 of `foo` should do [desired behavior], but is doing [erroneous behavior].\"\n",
      " - Example hypothesis 3: \"The previous hypothesis was supported. I think changing the code by changing `c>b` to `c>b && a <= d` may fix the code.\"\n",
      " - Example hypothesis 4: \"The previous experiment ended in an error. We may need to try a different experiment. Perhaps the experiment can be refined by inspecting the value of [expression] at line [line].\"\n",
      "\n",
      "Prediction: A specific value or symptom that would be observed if the hypothesis is correct. Depending on the hypothesis, one may make the prediction that a test will pass. Make specific considerations for expressions within loops.\n",
      "\n",
      " - Example 1: If I use the debugger to print [expr], while given the input and its intended role indicates that its value should be [desired value], it will be [erroneous value]; that is, when I stop the debugger at line lineno, `[expr] == [actual_value]` will be true.\n",
      " - Example 2: If I change [expr] to [new_expr], the test will pass.\n",
      "\n",
      "Experiment: A specific script that would check whether the prediction made is true. Make sure the line points to an actual statement (not a bracket).\n",
      "\n",
      " - Example 1 (pdb script): `b debugging.py:lineno ;; c ;; p x != v`\n",
      " - Example 2 (edit script): `REPLACE(4321, \"c>b\", \"c>b && a <= d\") AND RUN`\n",
      "\n",
      "Observation: The output of the script. Example: `False`\n",
      "\n",
      "Conclusion: A judgement whether the hypothesis is true based on the observation. Also add <DEBUGGING DONE> when the hypothesis confirmed leads to a concrete program fix. \n",
      " - Example: The hypothesis is (supported/rejected/undecided due to experiment error). [When a test passed, add <DEBUGGING DONE>.]\n",
      "\n",
      "## Description\n",
      "\n",
      "I have the following python function and test (at debugging.py)\n",
      "\n",
      "```python\n",
      "1 from typing import List, Optional\n",
      "2 \n",
      "3 \n",
      "4 def longest(strings: List[str]) -> Optional[str]:\n",
      "5     \"\"\" Out of list of strings, return the longest one. Return the first one in case of multiple\n",
      "6     strings of the same length. Return None in case the input list is empty.\n",
      "7     >>> longest([])\n",
      "8 \n",
      "9     >>> longest(['a', 'b', 'c'])\n",
      "10     'a'\n",
      "11     >>> longest(['a', 'bb', 'ccc'])\n",
      "12     'ccc'\n",
      "13     \"\"\"\n",
      "14     if not strings:\n",
      "15         return None\n",
      "16     maxlen = max((len(x) for x in strings))\n",
      "17     for s in strings:\n",
      "18         return s\n",
      "19 assert longest(['x', 'yyy', 'zzzz', 'www', 'kkkk', 'abc']) == 'zzzz', longest(['x', 'yyy', 'zzzz', 'www', 'kkkk', 'abc'])\n",
      "```\n",
      "\n",
      "which fails with the error message\n",
      "```\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sungmin/Documents/work/humaneval-mutator/asd/debugging.py\", line 19, in <module>\n",
      "    assert longest(['x', 'yyy', 'zzzz', 'www', 'kkkk', 'abc']) == 'zzzz', longest(['x', 'yyy', 'zzzz', 'www', 'kkkk', 'abc'])\n",
      "AssertionError: x\n",
      "```\n",
      "\n",
      "## Analysis\n",
      "\n",
      "Using the scientific method, I debugged the issue as follows.\n",
      "\n",
      "Attempt 1.\n",
      "Hypothesis: Given that the function returns the first string it finds in the list, the function only returns the first string and does not iterate through the entire list.\n",
      "Prediction: If we change the return statement on line 18 to an if statement that only returns the string if its length is equal to the max length, the function will return the correct string.\n",
      "Experiment: `REPLACE(18, \"return s\", \"if len(s) == maxlen: return s\") AND RUN`\n",
      "Observation: `[No exception triggered]`\n",
      "Conclusion: The hypothesis is supported. <DEBUGGING DONE>\n",
      "\n",
      "The repaired code (full method, without comments) is:\n",
      "\n",
      "```python\n",
      "def longest(strings: List[str]) -> Optional[str]:\n",
      "    if not strings:\n",
      "        return None\n",
      "    maxlen = max((len(x) for x in strings))\n",
      "    for s in strings:\n",
      "        if len(s) == maxlen:\n",
      "            return s\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df.loc[1, 'prompt_at_repair'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LLM-Base results\n",
    "label = './results/chatgpt002_ZSnoSDloopV4_s0_out'\n",
    "full_info_df = get_result_for_label(label)\n",
    "full_info_df[full_info_df.autosd_repair_succ]['arhe_idx'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Codex-AutoSD\n",
    "label = './results/codex_ZSD_loopV4_s3_results'\n",
    "full_info_df = get_result_for_label(label)\n",
    "full_info_df[full_info_df.autosd_repair_succ]['arhe_idx'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Codex LLM-Base\n",
    "label = './results/codex_ZSnoSD_loopV4_s0_T0'\n",
    "full_info_df = get_result_for_label(label)\n",
    "full_info_df[full_info_df.autosd_repair_succ]['arhe_idx'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CodeGen-Big (6B) AutoSD\n",
    "label = './data/BigCG_ZSDloopV4_s3_results'\n",
    "full_info_df = get_result_for_label(label)\n",
    "full_info_df[full_info_df.autosd_repair_succ]['arhe_idx'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CodeGen-Big (6B) LLM-Base\n",
    "label = './data/autosd/bigcg_results/BigCG_ZSnoSD_s0_results'\n",
    "full_info_df = get_result_for_label(label, start_idx=3, end_idx=4)\n",
    "full_info_df[full_info_df.autosd_repair_succ]['arhe_idx'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fewshot CodeGen-Big (6B) LLM-Base\n",
    "label = './data/BigCG_FSnoSDnoDef_s0_results'\n",
    "full_info_df = get_result_for_label(label)\n",
    "full_info_df[full_info_df.autosd_repair_succ]['arhe_idx'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.77\n",
      "4.199654747714388\n"
     ]
    }
   ],
   "source": [
    "# reverse mutator baseline (implementation in reverse_mutator.py)\n",
    "import numpy as np\n",
    "base_perf = [83, 75, 85, 90, 82, 84, 82, 88, 79, 84, 86, 86, 93, 81, 85, 87, 81, 85, 92, 84, 95, 91, 87, 81, 93, 89, 84, 80, 89, 84, 86, 84, 85, 87, 83, 87, 90, 85, 82, 75, 84, 85, 88, 89, 91, 84, 93, 82, 82, 91, 83, 88, 77, 89, 82, 85, 88, 92, 91, 92, 84, 89, 87, 89, 93, 88, 89, 88, 85, 85, 78, 86, 85, 90, 86, 82, 84, 80, 87, 89, 91, 82, 89, 77, 88, 89, 88, 82, 91, 84, 86, 88, 81, 80, 79, 81, 89, 85, 88, 90]\n",
    "print(np.mean(base_perf))\n",
    "print(np.std(base_perf))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
